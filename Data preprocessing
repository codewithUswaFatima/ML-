🧹 What is Data Preprocessing?
Data Preprocessing is the first and most important step in a machine learning pipeline. It's all about cleaning and transforming raw data into a format that your machine learning model can understand and learn from effectively.

Think of it like this:
If your data is messy, noisy, or incomplete, your model is going to learn garbage — this is often called "Garbage in, garbage out."

🛠️ Why is it important?
Models can’t handle missing or inconsistent data

Models perform better with clean, normalized, and scaled input

Helps in removing noise and highlighting patterns

🔄 Steps in Data Preprocessing
1. Data Cleaning
Handle missing values

python
Copy
Edit
df.fillna(0)          # Replace with 0
df.dropna()           # Drop rows with missing values
Fix incorrect or inconsistent entries

Remove duplicates

2. Data Transformation
Scaling / Normalization
Ensures all features are on a similar scale.

python
Copy
Edit
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)
Encoding categorical variables Convert text to numbers.

python
Copy
Edit
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['gender'] = le.fit_transform(df['gender'])  # 'male' -> 1, 'female' -> 0
3. Feature Selection / Extraction
Keep only the most important features for the model

Create new features from existing ones (like age_group from age)

4. Data Splitting
Split data into training and testing sets.

python
Copy
Edit
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
5. Handling Outliers
Outliers can distort your model

Use methods like z-score or IQR filtering to remove them

🎯 Example in Context
Imagine you’re training a model to predict house prices:

If a row is missing the number of bedrooms, the model won’t work well.

If the prices range from $10k to $1M, normalization helps scale them down.

If the "Location" column is text, it needs to be encoded into numbers.

